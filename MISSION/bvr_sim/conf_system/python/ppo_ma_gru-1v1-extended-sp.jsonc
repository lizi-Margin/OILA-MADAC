{
    // ========================================================================
    // BVR 3D - 1v1 Training Configuration
    // PPO with GRU for 3D Beyond Visual Range Air Combat
    // ========================================================================

    // ===================== UHRL Framework Core Config =====================
    "config.py->GlobalConfig": {
        "env_name": "bvr_sim",
        "logdir": "RESULT/bvr_sim-ppo_ma_gru-1v1-extended-sp",
        "seed": 1111,
        "num_threads": 32,
        "fold": 1,
        "report_reward_interval": 32,
        "max_n_episode": 500000
    },

    // ===================== BVR 3D Environment Config =====================
    "MISSION.bvr_sim.env_wrapper->ScenarioConfig": {
        // Team configuration
        // "AGENT_ID_EACH_TEAM": [[0]],  // Red team (RL agent) vs rule-based
        "AGENT_ID_EACH_TEAM": [[0], [1]],  // Red team (PPO-GRU) vs Blue team (NSP self-play)
        "TEAM_NAMES": [
            "ALGORITHM.ppo_ma_gru.foundation->ReinforceAlgorithmFoundation",
            "ALGORITHM.ppo_ma_gru_nsp.foundation->ReinforceAlgorithmFoundation"
        ],
        "interested_team": 0,  // Train red team (PPO-GRU)

        // Episode configuration
        "MaxEpisodeStep": 1000,   
        "dt": 0.4, 

        // Scenario configuration
        "field_size": 100000.0,  // 100 km battle space

        // Spawn configuration (randomized positions)
        "initial_separation_nm": 37.2,  // Initial distance between teams (nautical miles)
        "formation_max_spread_nm": 2.0,  // Maximum formation spread (nautical miles)

        // Observation space configuration
        "EntityOriented": false,
        "obs_type": "extended",  // 'compact' (47 dims) or 'extended' (73 dims)
        // Compact obs for 1v1: 9 (self) + 10 (enemy) + 0 (allies) + 28 (missiles) = 47
        "obs_dim": 75,
        "obs_shape": [75],

        // Blue team opponent
        // "blue_opponent_type": "tactical",  // str or list of ['random','simple','tactical']
        // "blue_opponent_type": ["random", "simple", "mad", "tactical"],
        // "blue_opponent_type": ["simple", "mad", "tactical"],
        // "blue_opponent_type": ["mad", "tactical"],
        // "blue_opponent_type": "tactical",
        "blue_opponent_type": null,  // null = both teams RL-controlled (self-play)

        // Rendering configuration
        "render": true,  // Enable for ACMI recording (set to true for visualization)
        "render_interval": 1  // Render every step (ACMI is fast)

    },

    // --- config ALGORITHM 1/2 ---
    "ALGORITHM.ppo_ma_gru.foundation->AlgorithmConfig": { // must kiss with "TEAM_NAMES"
        "train_traj_needed": 32,
        "use_normalization": true,
        "load_checkpoint": true,
        // "load_specific_checkpoint": "",
        "gamma": 0.99,
        "lr": 3e-4,
        "entropy_coef": 0.0001,
        "ppo_epoch": 16,
        "net_hdim": 128,

        "device": "cuda"
    },
    "uhtk.UTIL.tensor_ops->ConfigCache": {
        "device_to_config": "cuda"
    },


    // --- config ALGORITHM 2/2 ---
    "ALGORITHM.ppo_ma_gru_nsp.foundation->AlgorithmConfig": {
        "load_checkpoint": true,
        "net_hdim": 128,
        "device": "cuda",

        // Policy zoo configuration
        "use_policy_zoo": true,  // Enable policy zoo mechanism
        "policy_zoo_size": 10,  // Keep last 10 checkpoints in zoo
        "policy_update_interval": 5,  // Update policy every 5 training iterations
        "policy_zoo_prob_latest": 0.3,  // 30% chance of using latest, 70% random from zoo
        "policy_zoo_save_interval": 50  // PPO saves checkpoint every 50 updates (don't change)
    },
    "ALGORITHM.ppo_ma_gru_nsp.tensor_ops->ConfigCache": {
        "device_to_config": "cuda"
    }
}
